---
layout:     post
title:      Spark RPC 详解
date:       2019-11-17
summary:    通过一个例子详细介绍了Spark RPC是如何工作的。
categories: spark
published:  true
---


Spark RPC作为Spark各个计算节点之间沟通的基础，其重要性不言而喻。而在深入了解spark的过程中，特别是阅读源码时，很多时候会碰到RPC相关代码，所以系统性地了解Spark RPC将会助你更好地理解Spark，让你更轻松愉快地阅读Spark源码，让你过上幸福的码农生活。


## Spark RPC Echo 例子
这里先通过Spark RPC实现一个echo的客户端/服务端程序来了解下Spark RPC时如何被使用的。

先构建一个EchoRpcEndpoint, 所有的功能实现都会放在这个类里面，可以看到它扩张了RpcEndpoint, 实现了receiveAndReply等几个方法，可以看到Echo的方法就是在这里面实现的。另外我们这里实现的类全部都要放在package  org.apache.spark下面，这是因为这些类都有scala package级别的限制，必须要在org.apache.spark下面。

```scala
package org.apache.spark.demo

import org.apache.spark.rpc.{RpcCallContext, RpcEndpoint, RpcEnv}

class EchoRpcEndpoint(rpc: RpcEnv) extends RpcEndpoint {

  override val rpcEnv: RpcEnv = rpc

  private var inc: Int = 0

  override def onStart(): Unit = {
    println(s"${this.getClass.getName} started")
  }

  override def onStop(): Unit = {
    println(s"${this.getClass.getName} stopped")
  }

  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {
    case EchoMessage(msg) =>
      inc = inc + 1
      println(s"messaged received:$msg")
      context.reply(EchoMessage(s"$msg ${(0 until inc).map(_ =>"+").mkString("")}"))
  }

}

object EchoRpcEndpoint {
  val ENDPOINT_NAME = "echo"
}
```

再来构建一个服务端, 主要分两个步骤
1. 用RpcEnv.create方法来构建 - 不管是客户端还是服务端，都需要一个RpcEnv，当指定clientMode=false，同时兼具服务端和客户端功能。在Spark里面，不管是Executor还是Driver都会创建一个RpcEnv, 2.x之后版本的实现是NettyRpcEnv，即底层通信是通过netty.io这个包来完成。
2. 注册EchoRpcEndpoint - 具体的功能实现都放在RpcEndpoint里面，并且要在RPCEnv里面去注册这个RpcEndpoint, 这样子才能接受处理客户端的请求，另外每个endpoint都有自己的名字。
    ```scala
    def process(): Unit ={
        val sparkConf = new SparkConf()
        val securityManager = new SecurityManager(sparkConf)
        val rpcEnv = RpcEnv.create("test-env", "127.0.0.1", 10000, sparkConf, securityManager, clientMode = false)
        val echoRpcEndpointRef = rpcEnv.setupEndpoint(EchoRpcEndpoint.ENDPOINT_NAME, new EchoRpcEndpoint(rpcEnv))

        Thread.sleep(100000)
        rpcEnv.stop(echoRpcEndpointRef)
        rpcEnv.shutdown()
    }
    ```


最后构建一个客户端，代码跟服务端非常相似，只是没有服务的启动，所以这里端口指定了-1，host指定了空字符串。在创建之后，需要设置一个RpcEndpointRef，作为服务端注册过的RpcEndpoint的引用，这里看到指定了ip和端口并且还有一个名字，也就是说这3个要素可以唯一指定一个RpcEndpoint。设置之后就可以通过ask等方法向处于远程的RpcEndpoint发送消息了。

```scala
def process(): Unit ={
  val sparkConf = new SparkConf()
  val securityManager = new SecurityManager(sparkConf)

  val rpcEnv = RpcEnv.create("test-env", "", -1, sparkConf, securityManager, clientMode = true)

  val ref = rpcEnv.setupEndpointRef(RpcAddress("127.0.0.1",10000), EchoRpcEndpoint.ENDPOINT_NAME)

  0 until 10 foreach{ i =>
    val reply = ref.askSync[EchoMessage](EchoMessage("hallo"))
    println(s"message replied: $reply")
  }
  rpcEnv.stop(ref)
  rpcEnv.shutdown()
}
```



可以看到Spark RPC使用起来也不是那么难，实现一个RpcEndpoint, 创建服务端版本RpcEnv, 注册RpcEndpoint，创建客户端版RpcEnv，设置一个RPcEndpointRef，然后用Ref来向远处发送消息。可以看到整个RPC系统更类似于一个远处消息传递系统，而不是像hadoop rpc那样，看起来像是调用本地的方法，其实调用的远处方法。这可能是因为早期的Spark RPC是用Akka实现的缘故。

代码地址 - https://github.com/zhongjinhan/demo/tree/master/spark-rpc


就上面这个例子，我们要看下底层到底如何工作的，先看下服务端的情况。


<br/>
## 服务度详解

<br/>
**服务的创建**

RpcEnv的具体实现NettyRpcEnv，是通过RpcEnvFactory来创建的，这个Factory的具体实现是NettyRpcEnvFactory，所以在NettyRpcEnvFactory#create里面，我们可以看到，如果clientMode=false，则会调用Utils.startServiceOnPort这样一个通用服务启动方法来启动NettyRpcEnv#startServer方法。Utils.startServiceOnPort里面主要封装了一些重试机制(端口会递增）和异常处理，这是一个很多地方都用到的方法，比如Spark客户端启动的时候会打开4040的http服务，如果4040端口被占用，则尝试4041。


```scala
def create(config: RpcEnvConfig): RpcEnv = {
  val sparkConf = config.conf
  // Use JavaSerializerInstance in multiple threads is safe. However, if we plan to support
  // KryoSerializer in future, we have to use ThreadLocal to store SerializerInstance
  val javaSerializerInstance =
    new JavaSerializer(sparkConf).newInstance().asInstanceOf[JavaSerializerInstance]
  val nettyEnv =
    new NettyRpcEnv(sparkConf, javaSerializerInstance, config.advertiseAddress,
      config.securityManager, config.numUsableCores)
  if (!config.clientMode) {
    val startNettyRpcEnv: Int => (NettyRpcEnv, Int) = { actualPort =>
      nettyEnv.startServer(config.bindAddress, actualPort)
      (nettyEnv, nettyEnv.address.port)
    }
    try {
      Utils.startServiceOnPort(config.port, startNettyRpcEnv, sparkConf, config.name)._1
    } catch {
      case NonFatal(e) =>
        nettyEnv.shutdown()
        throw e
    }
  }
}
```

NettyRpcEnv#startServer会调用TransportContext#createServer方法去初始化一个TransportServer，在TransportServer的init方法里面可以看到使用了netty的ServerBootstrap等API创建了一个服务，而在使用netty.io这个库的时候，我们知道大部分逻辑实现都会放在ChannelHandler里面，多个channel handler会串成一条pipeline， 而这里的channelHandle的添加是在TransportContext#initializePipeline里面进行的，可以看到依次添加了以下Handler
1. MessageEncoder - 顾名思义，就是消息编码器，对服务端到客户端的返回消息进行编码，也就是序列化操作，可是为什么这个会放在pipeline的第一个位置呢？这主要是netty的handler 分为outbound和inbound， 接收到的数据会依次通过各个inbound，返回的数据会通过当前inbound之前的所有outbound的handler，因为decoder是一个outbound handler，并且出去的所有消息都必须要序列化，所以就放在第一个了。
2. TransportFrameDecoder - 在对具体消息进行解码之前，对裸数据进行帧解码。这个主要是对socket过来的碎片化的字节按规定进行重新合并，以便后续应用方便处理，是一个inbound handler。
3. MessageEncoder - 一个 inbound handler， 对帧解码后的字节数组进行解码，按消息类型分别解析成不同类型的消息，
4. IdleStateHandler - netty提供的一个空闲状态 handler，是一个双向 handler。
5. TransportChannelHandler  - 一个inbound handler，主要的服务端处理逻辑都在这个handler里面。

```scala
public TransportChannelHandler initializePipeline(
    SocketChannel channel,
    RpcHandler channelRpcHandler) {
  try {
    TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler);
    channel.pipeline()
      .addLast("encoder", ENCODER)
      .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder())
      .addLast("decoder", DECODER)
      .addLast("idleStateHandler", new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))
      // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this
      // would require more logic to guarantee if this were not part of the same event loop.
      .addLast("handler", channelHandler);
    return channelHandler;
  } catch (RuntimeException e) {
    logger.error("Error while initializing Netty pipeline", e);
    throw e;
  }
}
```


**Rpc请求的接收**

消息处理的入口点是TransportChannelHandler， 因为这个handler不仅被服务端使用也被客户端使用，所以TransportChannelHandler包含TransportResponseHandler和TransportRequestHandler，在TransportChannelHandler#channelRead方法里面会根据消息的类型分别调用相应的hanlder的handle方法，当这里先考虑TransportRequestHandler。

```scala
public void channelRead(ChannelHandlerContext ctx, Object request) throws Exception {
  if (request instanceof RequestMessage) {
    requestHandler.handle((RequestMessage) request);
  } else if (request instanceof ResponseMessage) {
    responseHandler.handle((ResponseMessage) request);
  } else {
    ctx.fireChannelRead(request);
  }
}
```

TransportRequestHandler#handle里面会更进一步根据request消息的类型去调用相应的处理方法, 这里我们只考虑RpcRequest，对应的方法是processRpcRequest，在这里面主要调用了NettyRpcHandler的receive方法。在receive里面先调用internalReceive方法生成RequestMessage，生成RequstMessage的过程中会对消息的内容进行反序列化操作，这是在RequestMessage#apply方法里面调用NettyRpcEnv#deserialize完成的。最后调用dispatcher的postRemoteMessage。
```scala
override def receive(
    client: TransportClient,
    message: ByteBuffer,
    callback: RpcResponseCallback): Unit = {
  val messageToDispatch = internalReceive(client, message)
  dispatcher.postRemoteMessage(messageToDispatch, callback)
}
```


**Rpc请求的分发**

请求的分发主要是通过Dispatcher, 是在NettyRpcEnv初始化时被创建的。Dispathcer里面主要有
1. endpoints - 用来存储EndpointData，当RpcEdnpoint在RpcEnv注册的时候，RpcEndpoint会被添加到这里面来。当在服务端被创建之后，一定会注册一个默认的RpcEndpoint - RpcEndpointVerifier， 这是用来验证RpcEdnpoint的比较特殊的RpcEndpoint。
2. endpointRefs - 用来存储RpcEdnpoint对应的RpcEndpointRef, 也是在RpcEndpoint注册的时候被添加进来。
3. MessageLoops - Dispatcher创建时会启动一组线程池运行MessageLoop, 线程个数由spark.rpc.netty.dispatcher.numThreads控制， 这组线程池会不断处理Dispatcher里面的消息队列的数据(receiver)。

接着上面的流程，NettyRpcHandler会调用Dispatcher#postRemoteMessage， 这里面会用原有消息构建RpcMessage，然后再调用postMessage, 在这里面。用endpointName从endpoints获取EndpointData，然后向相应的ednpoint所对应的Inbox里面post一条消息，然后再向receivers这个队列发送一个endpointdata。
private def postMessage(
    endpointName: String,
    message: InboxMessage,
    callbackIfStopped: (Exception) => Unit): Unit = {
  val error = synchronized {
    val data = endpoints.get(endpointName)
    if (stopped) {
      Some(new RpcEnvStoppedException())
    } else if (data == null) {
      Some(new SparkException(s"Could not find $endpointName."))
    } else {
      data.inbox.post(message)
      receivers.offer(data)
      None
    }
  }
  // We don't need to call `onStop` in the `synchronized` block
  error.foreach(callbackIfStopped)
}


**Rpc请求的处理**

当一个MessageLoop获取到一个EndpointData的时候，会调用这个EndpointData的Inbox#process方法处理请求，在这里面会调用相应Endpoint所实现的receiveAndReply方法来处理请求。



**返回处理后的结果**

在TransportRequestHandler#processRpcRequest里面会生成一个RpcResponseCallback，然后传入NettyRpcHandler#receive方法，后面会在Dispatcher#postRemoteMessage里面用这个callback函数生成RemoteNettyRpcCallContext，最后在实现RpcEndpoint#receiveAndReply的时候调用RemoteNettyRpcCallContext#reply方法返回结果, 在这个方法里面先调用NettyRpcEnv#serialize方法来序列化返回结果，然后再调用callback的onSuccess方法。
```scala
private[netty] class RemoteNettyRpcCallContext(
    nettyEnv: NettyRpcEnv,
    callback: RpcResponseCallback,
    senderAddress: RpcAddress)
  extends NettyRpcCallContext(senderAddress) {

  override protected def send(message: Any): Unit = {
    val reply = nettyEnv.serialize(message)
    callback.onSuccess(reply)
  }
}
```

RpcResponseCallback#onSuccess方法会调用TransportRequestHandler#respond方法， 在这里面会调用SocketChannel的writeAndFlush方法把result写入到socketchannel，当然这个消息也会经过MessageEncoder和个handler。
```scala
private ChannelFuture respond(Encodable result) {
  SocketAddress remoteAddress = channel.remoteAddress();
  return channel.writeAndFlush(result).addListener(future -> {
    if (future.isSuccess()) {
      logger.trace("Sent result {} to client {}", result, remoteAddress);
    } else {
      logger.error(String.format("Error sending result %s to %s; closing connection",
        result, remoteAddress), future.cause());
      channel.close();
    }
  });
}
```



<br/>

## 客户端详解

**RpcEnv客户端的创建**

在调用NettyRpcEnvFactory#create创建RpcEnv的时候，如果clientMode=true, 那么就相当于创建客户端，跟服务端模式的RpcEnv的差别在于没有创建服务。


**注册RpcEndpointRef**

我们的目的是调用远端的RpcEndpoint,在Spark RPc里面用RpcEndpointRef来代表一个元端的RpcEndpoint的引用。可以通过RpcEnv#setupEndpointRef来创建一个远端的RpcEndpoint。NettyRpcEndpointRef是RpcEndpointRef的具体实现，在setupEndpointRef的过程中在NettyRpcEnv#asyncSetupEndpointRefByURI里面被创建的。在这里面同时还会创建一个RpcEndpointVerifier的RpcEndpointRef，然后发送一个RpcEndpointVerifier.CheckExistence的消息，用来验证当前创建的RpcEndpointRef是否存在，这个RpcEndpointVerifier在上面的服务器端也提到过，在服务器端是被默认注册的。

```scala
def asyncSetupEndpointRefByURI(uri: String): Future[RpcEndpointRef] = {
  val addr = RpcEndpointAddress(uri)
  val endpointRef = new NettyRpcEndpointRef(conf, addr, this)
  val verifier = new NettyRpcEndpointRef(
    conf, RpcEndpointAddress(addr.rpcAddress, RpcEndpointVerifier.NAME), this)
  verifier.ask[Boolean](RpcEndpointVerifier.CheckExistence(endpointRef.name)).flatMap { find =>
    if (find) {
      Future.successful(endpointRef)
    } else {
      Future.failed(new RpcEndpointNotFoundException(uri))
    }
  }(ThreadUtils.sameThread)
}
```

**Rpc消息的发送**

RPc消息的发送可以通过RpcEndpointRef的askSync完成，askSync最后会调用NettyRpcEnv#ask方法

```scala
override def ask[T: ClassTag](message: Any, timeout: RpcTimeout): Future[T] = {
  nettyEnv.ask(new RequestMessage(nettyEnv.address, this, message), timeout)
}
```


在NettyRpcEnv#ask里面，有两种情况，我们这里只考虑远程发送，所以会序列化消息，然后调用postToOutbox

```scala
if (remoteAddr == address) {
  val p = Promise[Any]()
  p.future.onComplete {
    case Success(response) => onSuccess(response)
    case Failure(e) => onFailure(e)
  }(ThreadUtils.sameThread)
  dispatcher.postLocalMessage(message, p)
} else {
  val rpcMessage = RpcOutboxMessage(message.serialize(this),
    onFailure,
    (client, response) => onSuccess(deserialize[Any](client, response)))
  postToOutbox(message.receiver, rpcMessage)
  promise.future.failed.foreach {
    case _: TimeoutException => rpcMessage.onTimeout()
    case _ =>
  }(ThreadUtils.sameThread)
}

```



在postToOutbox(message.receiver, rpcMessage)里面，会从outboxes获取一个outbox，如果没有，新建一个后然调用Outbox#send，把消息加到这个Outbox的消息队列messages(链表)里面，然后调用Outbox#drainOutbox，在这里面，如果connection还没建立，调用launchConnectTask建立client connection，launchConnectTask会另外起线程建立连接，所以这里直接返回了。

```scala
private def drainOutbox(): Unit = {
  var message: OutboxMessage = null
  synchronized {
    if (stopped) {
      return
    }
    if (connectFuture != null) {
      // We are connecting to the remote address, so just exit
      return
    }
    if (client == null) {
      // There is no connect task but client is null, so we need to launch the connect task.
      launchConnectTask()
      return
    }
    if (draining) {
      // There is some thread draining, so just exit
      return
    }
    message = messages.poll()
    if (message == null) {
      return
    }
    draining = true
  }
  while (true) {
    try {
      val _client = synchronized { client }
      if (_client != null) {
        message.sendWith(_client)
      } else {
        assert(stopped == true)
      }
    } catch {
      case NonFatal(e) =>
        handleNetworkFailure(e)
        return
    }
    synchronized {
      if (stopped) {
        return
      }
      message = messages.poll()
      if (message == null) {
        draining = false
        return
      }
    }
  }
```


Outbox#launchConnectTask在线程里面调用NettyRpcEnv#createClient建立到服务端的连接（TransportClient），等建立完成之后把connectFuture置空，表示连接建立完成，然后在这个线程里面再次调用drainOutbox， 这个时候因为到服务端的连接已经建立，所以会循环从消息队列中poll出消息，然后调用消息的sendWith方法，发送消息给服务端。


消息的最后发送是通过RpcOutboxMessage#sendWith完成，这里会调用TransportClient#sendRpc。在这里会先把这个rpcRequest通过TransportResponseHandler#addRpcRquest加入到处理reponse的handler里面，再调用ScoketChannel#.writeAndFlush发送消息。

```java
public long sendRpc(ByteBuffer message, RpcResponseCallback callback) {
  if (logger.isTraceEnabled()) {
    logger.trace("Sending RPC to {}", getRemoteAddress(channel));
  }

  long requestId = requestId();
  handler.addRpcRequest(requestId, callback);

  RpcChannelListener listener = new RpcChannelListener(requestId, callback);
  channel.writeAndFlush(new RpcRequest(requestId, new NioManagedBuffer(message)))
    .addListener(listener);

  return requestId;
}
```

**Rpc请求回复的处理**

会在TransportResponseHandler#handle里面处理返回的消息，这里会使用存储进来的callback函数来处理成功的消息，可以看到这个穿进去的callback方法里面还调用了NettyRpcEnv#deserialize来反序列回复。

```scala
val rpcMessage = RpcOutboxMessage(message.serialize(this),
  onFailure,
  (client, response) => onSuccess(deserialize[Any](client, response)))
```


<br/>
## 小结

Spark Rpc相对来说是比较简单的，网络这层直接使用现有的高性能框架netty，RPc消息的处理方面使用了很多scala语言本身的特性，很大程度上面简化了整个框架结构。

