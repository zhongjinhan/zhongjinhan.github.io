---
layout:     post
title:      HBase WAL实现详解
date:       2020-05-24
summary:    边看源码边详解
categories: hbase
published:  true
---




HBase WAL在HBase里面的角色跟关系型数据库里面的事务日志非常类似，比如MySQL的binlog或者Oracle的redolog，的主要目的是在Region Server崩掉之后在能够恢复落盘的数据从而保持数据的一致性。
当RegionSever接收到数据写入的时候，默认情况下会写一份
数据到WAL，当Region Server失败之后，master会重新分配并恢复这个rs上面的region到其他的region server，在这个过程中会去读取WAL并重新
写入到到对应的region，这里先看下WAL写入流程。


<br/>

## WAL写入流程

Write Ahead Log在Hbase里面的实现是FSHLog, 每个Region Server只有一个FSHLog实例，这意味着一个RegionServer里面的所有Region都会使用同一个实例
向同一个HDFS上面的日志文件写入，而不是一个Region单独一个Log，我想主要还是考虑到HDFS不适合这种多文件写入的场景。


<br/>
### FSHLog构建

FSHLog是由HRegionSever里面的WALFactory产生的，WALFactory由方法HRegionServer#setupWALAndReplication在RegionServer启动
的时候被调用,
这个方法初始化了WAL在HDFS上面对应的目录，同时也进行replication相关的设置。
HRegionServer#setupWALAndReplication返回的WALFactory里面有个WALProvider, 这
里默认使用了DefaultWALProvider。当RegionServer调用OpenRegionHandler#openRegion打开
Region的时候会调用HRegionServer#getWAL来获取一个WAL，最终会调用DefaultWALProvider#getWAL。
在DefaultWALProvider#getWAL里面构建了一个FSHLog并返回。



<br/>

### 日志写入

WAL的日志写入主要使用到了Disruptor框架，这个框架提供了一个基于环形buffer的高效数据处理流程，
各个数据写入进程会publish一个WALEntry到这个环形buffer里面，后面有个进程在消费，主要流程如下图所示：
![]({{ "/images/hbase-wal-write.png" | absolute_url }})



步骤描述如下：
1. 向Ringbuffer发布FSWALEntry或者SyncFuture：这里有两种数据要向Ringbuffer发布，分别是FSWALEntry和SyncFuture。
FSWALEntry里面包含写入的数据和一些元数据信息，通常是一个put操作，也就是一条Row的数据，可以包含多个Cell，这个publish操作是在FSHLog#append执行的。
SyncFuture则是代表一个HDFS的flush落盘操作，同一个线程会复用一个SyncFuture， 这个publish操作是在FSHLog#publishSyncOnRingBuffer里面执行。不管是
那种类似的操作，都是在Handler进程里面发起的，即RegionServer处理写入的RPC的时候发起的。

2. 从RingBuffer消费数据：这部分是由Disrupter框架完成的，消费的时候会调用RingBufferEventHandler#onEvent方法, 这里只有一个线程在消费，所以能保证sync
和append的顺序。RingBuffer的sequence从0开始一只递增，但是它的buffer大小是固定的,
 这里默认是1024*16，一旦消费来不及处理buffer里面的数据，发布端就会被block。

3. 把FSWALEntry写入HDFS的日志文件: 调用ProtobufLogWriter#append方法把FSWALEnetry写入到HDFS上面的日志文件。ProtobufLogWriter是在FSHLog构造时，
首次调用FSHLog#rollWriter方法的时候生成的，具体是通过DefaultWALProvider#createWriter方法创建并调用ProtobufLogWriter#init进行初始化。

4. 批量发送SyncFuture到SyncRunner的队列：RingBufferEventHandler里面会尽量以batch的形式发送SyncFuture到SyncRunner，每个批次默认大小是200，由参数
hbase.regionserver.handler.count控制。RingBufferEventHandler会启动多个SyncRunner线程，默认数量是5个，由参数hbase.regionserver.hlog.syncer.count
控制，每次会以轮询的方式找到一个SyncRunner，然后调用SyncRunner#offer方法向队列发送SyncFuture。

5. 从队列取出SyncFuture并处理：这是在SyncRunner的run方法里面处理的，先从队列中取出SyncFuture，然后调用
ProtobufLogWriter#sync方法对HDFS进行一个flush操作，即把缓冲里面的数据落到HDFS上面，最后更新highestSyncedSequence。因为这个操作相对来说是比较重的，
能避免重复执行当然是最好的，所以
在这之前会把当前的sequence跟highestSyncedSequence对比，如果小于这个值，就直接跳过这次sync操作，这种情况的发生主要是因为由多个SyncRunner的存在。







<br/>


## LogRoller


LogRoller的主要作用是周期性地运行查看WAL是否需要roll一下，就是重新写一个日志文件。
LogRoller在Regionserver的构建函数里面被构建，并且在HRegionServer#startServiceThreads里面启动。

滚动的触发有两种形式
1. 以WALActionsListener的形式注册到FSHLog里面，具体是在RegionOpen的时候调用HRegionServer#getWAL获取FSHLog的时候，会调用
LogRoller#addWAL生成一个匿名Listener然后，注册到FSHLog里面。
2. 以RPC的形式，即调用RSRpcServices#rollWALWriter 



Listener的触发有两个方法，分别是：
1. FSHLog#requestLogRoll， 基本上是在流程发生异常的情况下触发
2. FSHLog#checkLogRoll, 每次SyncRunner里面的sync动作执行完成之后都会调用这个方法，主要是检查
WAL日志文件的长度是否超过预定的文件长度，这个长度由
hbase.regionserver.logroll.multiplier乘以hbase.regionserver.hlog.blocksize所得，默认是0.95 * HDFS block size，如果超过了
，就触发滚动，基本上日志的滚动都是在这种情况下触发。



具体是怎么roll的是在FSHLog#rollWriter方法里面，这个方法在RegionServer启动的时候也会被调用用来初始化writer。滚动的时候基本上就是创建一个
新的文件然后把旧的文件替换掉，然后重置设置writer。
```java
public byte [][] rollWriter(boolean force) throws FailedLogCloseException, IOException {
  rollWriterLock.lock();
  try {
    if (!force && (this.writer != null && this.numEntries.get() <= 0)) return null;
    byte [][] regionsToFlush = null;
    if (this.closed) {
      LOG.debug("WAL closed. Skipping rolling of writer");
      return regionsToFlush;
    }
    if (!closeBarrier.beginOp()) {
      LOG.debug("WAL closing. Skipping rolling of writer");
      return regionsToFlush;
    }
    TraceScope scope = Trace.startSpan("FSHLog.rollWriter");
    try {
      Path oldPath = getOldPath();
      Path newPath = getNewPath();
      Writer nextWriter = this.createWriterInstance(newPath);
      FSDataOutputStream nextHdfsOut = null;
      if (nextWriter instanceof ProtobufLogWriter) {
        nextHdfsOut = ((ProtobufLogWriter)nextWriter).getStream();
        preemptiveSync((ProtobufLogWriter)nextWriter);
      }
      tellListenersAboutPreLogRoll(oldPath, newPath);
      newPath = replaceWriter(oldPath, newPath, nextWriter, nextHdfsOut);
      tellListenersAboutPostLogRoll(oldPath, newPath);
      if (getNumRolledLogFiles() > 0) {
        cleanOldLogs();
        regionsToFlush = findRegionsToForceFlush();
      }
    } finally {
      closeBarrier.endOp();
      assert scope == NullScope.INSTANCE || !scope.isDetached();
      scope.close();
    }
    return regionsToFlush;
  } finally {
    rollWriterLock.unlock();
  }
}
```











<br/>

## 日志切分

当一个RegionSever节点崩掉以后，HBase会尝试重新分配这个Server上面的Region到其他可用的RegionServer上面，
因为每个RegionServer只有一个WAL文件，而重新分配的Region可能会被分配到不同的RegionServer上面，所以这时候就要先把
这个WAL文件按region切分。

日志切分是通过zookeeper的znode来分发到不同的region server上面执行的，大体流程如下：



![]({{ "/images/hbase-wal-split.png" | absolute_url }})

每个步骤的详细流程如下：
1. 提交splitTask: 当RegionServer失败之后， zookeoper上面对应的临时znode会自动被删除，这时候会触发RegionServerTracker#nodeDeleted，
继而调用ServerManager#expireServer来让这个regionserver失效，这里面会提交一个
ServerCrashProcedure的proceudre。ServerCrashProcedure是用来恢复这个崩掉的Regionserver上面的所有region，里面包含了多个步骤，
其中有一个就是SERVER_CRASH_SPLIT_LOGS, 这个步骤会调用ServerCrashProcedure#splitLogs,继而调用SplitLogManager#splitLogDistributed
来切分日志。接着会调用SplitLogManager#enqueueSplitTask去构建splitTask然后提交task到ZKSplitLogManagerCoordination
2. 在zookeeper上面创建task信息：ZKSplitLogManagerCoordination会调用createNode在/hbase/splitWAL下面创建znode，里面包含了splittask信息。
3. 从Zookeeper上获取task信息：日志切分的工作主要是在RegionServer里面完成的，每个RegionServer在启动的时候都会构建一个SplitLogWorker, 并启动它。SplitLogWorker线程的主要工作是
调用ZkSplitLogWorkerCoordination#taskLoop，在这循环里面首先会调用ZkSplitLogWorkerCoordination#getTaskList去zookeeper的/hbase/splitWAL下面
尝试获取task信息。
4. 解析任务并构建提交WALSplitterHandler：获取到task信息之后会进入方法ZkSplitLogWorkerCoordination#grabTask，在这里面会先调用SplitLogTask#parseFrom去解析任务，然后在
ZkSplitLogWorkerCoordination#submitTask里面构建一个WALSplitterHandler，并提交到类型为EventType.RS_LOG_REPLAY的ExecutorService。

5. 发送RPC(getLastFlushedSequenceId)到Master: 在ExecutorService里面会执行WALSplitterHandler#process方法，这里面会执行SplitLogWorker.TaskExecutor的exec方法，这个taskExecutor是在SplitLogWorker
的构建方法里面生成的一个匿名taskExecutor，里面调用WALSplitter#splitLogFile方法来切分日志。在splitLogFile里面会先调用HRegionServer#getLastSequenceId，继而调用RPC去master里面获取sequence信息，master里面对应RPC处理方法是
MasterRpcServices#getLastFlushedSequenceId。 之所要在这里加上这个步骤是因为这个信息比较关键，因为flushed sequence代表一个region里面的每个
memstore里面哪些数据已经落在持久到磁盘上面，即转换成HFile。

6. Master返回LastFlushedSequenceId信息： Master里面的RPC handler调用ServerManager#getLastFlushedSequenceId获取对应region的flush sequence信息并返回。
ServerManager里面会存储每个region以及每个region里面的store的最近flush的sequenceid，这部分信息是由region server 报告给master的。
RegionSever会在主循环RegionServer#run方法每隔3秒(默认)调用HRegionServer#tryRegionServerReport方法向master报告，
报告所用到的信息定义是ClusterStatusProtos.ServerLoad，这里面就包含了flushsequence信息, 针对每个region，会调用HRegion#setCompleteSequenceId设置sequqnce信息。
ServerLoad是由方法HRegionServer#buildServerLoad构建，构建完成之后调用RPC调用RegionServerStatusService.BlockingInterface#regionServerReport
向master发送请求。master接收之后调用org.apache.hadoop.hbase.master.MasterRpcServices#regionServerReport来处理， 里面会调用org.apache.hadoop.hbase.master.ServerManager#updateLastFlushedSequenceIds
来更新这些sequence信息。

7. 读取原始WAL并跟对比LastFlushedSequenceId：创建一个ProtobufLogReader并从WAL里面读取数据，然后跟flushedSequence对比，如果现有的sequence比这个小，直接过滤掉。


8. 追加数据到EntryBuffers: 调用EntryBuffers#appendEntry把过滤后的数据放到EntryBuffer里面。


9. 按Region写入文件： OutputSink里面会调用startWriterThreads方法创建多个WriterThread(默认是3个)，每个线程里面执行WALSplitter.WriterThread#doRun方法，这里会先调用EntryBuffers#getChunkToWrite方法去获取RegionEntryBuffer。
接着调用LogRecoveredEditsOutputSink#append方法，在这里面会调用LogRecoveredEditsOutputSink#getWriterAndPath为每个Region生成一个ProtobufLogWriter，也就是每个Region
一个WAL文件,最后调用ProtobufLogWriter#append把数据写到HDFS

10. 结束日志切分任务：等日志切分完成之后，在WALSplitterHandler#process里面会调用ZkSplitLogWorkerCoordination#endTask方法设置任务结束信息。

11. 在Zookeeper上面设置任务结束数据： 调用KUtil#setData在原来的任务znode上面设置结束的数据。
12. 接收任务结束信息： ZKSplitLogManagerCoordination监听到zookeeper上面的变化然后把结果反馈给SplitLogManager。





等日志切分完成之后，被重新分配的region就可以读取这些切分好的日志文件，然后apply这些变动。



-------
注：hbase代码基于cloudera hbase, 版本cdh5-1.2.0_5.13.0






