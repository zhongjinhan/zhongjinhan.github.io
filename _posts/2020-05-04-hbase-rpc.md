---
layout:     post
title:      HBase RPC
date:       2020-05-04
summary:    深入剖析HBase RPC
categories: hbase
published:  true
---



HBase RPC在0.96版本的时候进行了大改，改成基于protobuf的RPC框架。Protocal Buffer在这个框架里面起了两个作用

1. 定义了各种消息，包括RPC请求和答复的消息，RPC底层所涉及到的通用消息等等各类消息
2. 定义了服务以及RPC方法
另外消息的发送与接收以及底层网络通信分别由RpcClient和RpcServer来完成。

<br/>
## RPC消息与服务的定义

<br/>
**RPC消息**

RPC相关的消息定义全部在hbase-protocal这个子模块里面src/main/protobuf目录下面以.proto结尾的文件，比如我们熟悉的Admin#listTables这个RPC调用由请求与答复两个消息组成,所对应的消息定义在master.proto里面:

```
message GetTableDescriptorsRequest {
  repeated TableName table_names = 1;
  optional string regex = 2;
  optional bool include_sys_tables = 3 [default=false];
  optional string namespace = 4;
}

message GetTableDescriptorsResponse {
  repeated TableSchema table_schema = 1;
}
```


这两个消息定义在用protobuf compiler编译之后会生成两个对应的类:GetTableDescriptorRequest和GetTableDescriptorsResponse。这两个类定义在hbase-protocal的MasterProto.java里面。


<br/>

**RPC服务和方法**



RPC Service也是用protobuf来定义，比如以刚才Admin#listTables这个RPC，它的定义是在master.proto的MasterService里面：

```
service MasterService {
  /** Get list of TableDescriptors for requested tables. */
  rpc GetTableDescriptors(GetTableDescriptorsRequest)
    returns(GetTableDescriptorsResponse);
}
```

用protocalbuf生成的代码里面分别由两套接口与默认实现，分别针对blocking和非blocking，blocking对应的接口是MasterProtos.MasterService.BlockingInterface, 代码如下


```java
public org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsResponse getTableDescriptors(
    com.google.protobuf.RpcController controller,
    org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsRequest request)
    throws com.google.protobuf.ServiceException;
```


默认实现BlockingStub的实现如下，这里面只是简单地把调用转发到RpcChannel, RpcChannel是初始化BlockingStub时候传入的必要参数

```java
public org.apache.hadoop.hbase.protobuf.generated.MasterProtos.GetTableDescriptorsResponse getTableDescriptors(
    RpcController controller,
    MasterProtos.GetTableDescriptorsRequest request)
    throws com.google.protobuf.ServiceException {
  return (MasterProtos.GetTableDescriptorsResponse) channel.callBlockingMethod(
    getDescriptor().getMethods().get(1),
    controller,
    request,
    MasterProtos.GetTableDescriptorsResponse.getDefaultInstance());
}
```

<br/>

## 使用RPC消息与RPC服务

当使用protobuf工具基于这些消息与服务的定义生成了大量的java文件，下面看看HBase是如何利用这些类的。这里还是Admin#listTables这个面向Master的RPC调用为例。

HBaseAdmin#listTables的方法实现如下:
```java
public HTableDescriptor[] listTables() throws IOException {
  return listTables((Pattern)null, false);
}

  public HTableDescriptor[] listTables(final Pattern pattern, final boolean includeSysTables)
    throws IOException {
  return executeCallable(new MasterCallable<HTableDescriptor[]>(getConnection()) {
    @Override
    public HTableDescriptor[] call(int callTimeout) throws ServiceException {
      GetTableDescriptorsRequest req =
          RequestConverter.buildGetTableDescriptorsRequest(pattern, includeSysTables);
      return ProtobufUtil.getHTableDescriptorArray(master.getTableDescriptors(null, req));
    }
  });
}
```

可以看到里面定义了一个MasterCallable这个类并且覆盖了这个类的call方法，接着用getConnection获取一个ClusterConnection传入并初始化MasterCallable。

这个call方法里面涉及到了Rpc消息的创建与Rpc服务的使用，
1. 请求消息的构建: 调用RequestConverter.buildGetTableDescriptorsRequest
2. 服务接口的实现: master#getTabledescriptors里面的master其实是MasterKeepAliveConnection, 它扩展了protobuf自动生成的MasterService.BlockingInterface

下面来详细看看这两方面的实现


<br/>
**请求消息的构建**

在MasterCallable#call方法里面首先用过RequestConverter.buildGetTableDescriptorsRequest构建一个Request, RequestConverter是一个helper 工具专门用来构建protocal buffer 请求。可以看到buildGetTableDescriptorsRequest方法里面首先构建一个builder，然后分别设置了Regex和IncludeSystemTables两个属性，最后调用build方法生成GetTableDescriptorsRequest。


```java
public static GetTableDescriptorsRequest buildGetTableDescriptorsRequest(final Pattern pattern,
    boolean includeSysTables) {
  GetTableDescriptorsRequest.Builder builder = GetTableDescriptorsRequest.newBuilder();
  if (pattern != null) builder.setRegex(pattern.toString());
  builder.setIncludeSysTables(includeSysTables);
  return builder.build();
}
```



<br/>
**Service在客户端的使用**

MasterCallable里面的MasterKeepAliveConnection是调用HConnectionImplementation#getKeepAliveMasterService方法生成的一个动态实现, 比如getTableDescriptors的实现如下: 
```java
@Override
public GetTableDescriptorsResponse getTableDescriptors(
    RpcController controller, GetTableDescriptorsRequest request)
    throws ServiceException {
  return stub.getTableDescriptors(controller, request);
}
```

方法的实现只是简单地把调用转发到stub的getTableDescriptors方法。


从getKeepAliveMasterService里面也可以看到stub是调用MasterServiceStubMaker#makeStub方法生成，makeStub方法则调用了StubMaker#makeStubNoRetries，该方法的具体流程如下：
1. 首先获取了ZooKeeperKeepAliveConnection, 然后用它去获取master的地址。
2. 接着会去检查对应的stub有没有创建，这里用的key是由serviceName、host、port以及是否hostnamesCanChange组成，如果对应的stub还没创建，则先调用rpcClient.createBlockingRpcChannel创建一个RpcChannel, 具体的实现是BlockingRpcChannelImplementation
3. 最后用rpcChannel去创建stub，调用的是MasterServiceStubMaker#makeStub(channel), 里面只是简单调用了protobuf生成的MasterService.newBlockingStub(channel)方法来返回同样也是protobuf生成的默认实现MasterService.BlockingStub。


BlockingRpcChannelImplementation是在AbstractRpcClient里面创建，在BlockingRpcChannelImplementation#callBlockingMethod里面调用会进一步推送给AbstractRpcClient#callBlockingMethod, AbstractRpcClient#callBlockingMethod主要调用了RpcClientImpl#call方法, RpcClientImpl在后面会详细讲解。


所以总的来说，针对listTables对应的MasterService，protobuf所产生BlockingInterface是被MasterKeepAliveConnection扩展，默认实现
BlockingStub在MasterServiceStubMaker被使用，而具体的方法调用会被进一步推送到传入的BlockingRpcChannelImplementation，这个类实现了BlockingRpcChannel。最终方法的调用会被推送到RpcClientImpl#call, 由这个方法把调用传送到服务端执行。 


<br/>


**Service在服务端的使用**


MasterService.BlockingInterface在服务端的实现是MasterRpcServices。MasterRpcServices是HMaster的Rpc服务，它在HRegionServer的构建函数里面创建的，因为HMaster扩展了HRegionServer。HRegionSsrver调用了createRpcServices来创建，而这个方法在HMaster里面的实现是


```java
protected RSRpcServices createRpcServices() throws IOException {
  return new MasterRpcServices(this);
}
```

MasterRpcServices在初始化的时候会调用它的父类RSRpcServices的构造方法，在这里面会初始化RpcServer, 在这个过程中会调用getServices函数来生成一个servcie列表传入RpcServer, RegionSever里面的getServices包含了ClientService和AdminService, Master除此之外还包含了MasterService和RegionServerStatusService, 当然具体实现这些服务的方法都是在RSRpcServices和MasterRpcServices。

```java
protected List<BlockingServiceAndInterface> getServices() {
  List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(4);
  bssi.add(new BlockingServiceAndInterface(
    MasterService.newReflectiveBlockingService(this),
    MasterService.BlockingInterface.class));
  bssi.add(new BlockingServiceAndInterface(
    RegionServerStatusService.newReflectiveBlockingService(this),
    RegionServerStatusService.BlockingInterface.class));
  bssi.addAll(super.getServices());
  return bssi;
}

protected List<BlockingServiceAndInterface> getServices() {
  List<BlockingServiceAndInterface> bssi = new ArrayList<BlockingServiceAndInterface>(2);
  bssi.add(new BlockingServiceAndInterface(
    ClientService.newReflectiveBlockingService(this),
    ClientService.BlockingInterface.class));
  bssi.add(new BlockingServiceAndInterface(
    AdminService.newReflectiveBlockingService(this),
    AdminService.BlockingInterface.class));
  return bssi;
}
```

当客户端的RPC请求到达时，RpcServer会在这个列表里面去找到对应的service，详细情况在后面的RpcServer里面会讲解。


<br/>



**答复消息的构建**

答复消息的构建一般是在服务端的RPC Call的具体实现里面，比如说MasterRpcServices里面的getTableDescriptors就是服务端关于这个方法的实现，这个方法在返回结果前构建了GetTableDescriptorsResponse:


```java
public GetTableDescriptorsResponse getTableDescriptors(RpcController c,
    GetTableDescriptorsRequest req) throws ServiceException {
  try {
    master.checkInitialized();

    final String regex = req.hasRegex() ? req.getRegex() : null;
    final String namespace = req.hasNamespace() ? req.getNamespace() : null;
    List<TableName> tableNameList = null;
    if (req.getTableNamesCount() > 0) {
      tableNameList = new ArrayList<TableName>(req.getTableNamesCount());
      for (HBaseProtos.TableName tableNamePB: req.getTableNamesList()) {
        tableNameList.add(ProtobufUtil.toTableName(tableNamePB));
      }
    }

    List<HTableDescriptor> descriptors = master.listTableDescriptors(namespace, regex,
        tableNameList, req.getIncludeSysTables());

    GetTableDescriptorsResponse.Builder builder = GetTableDescriptorsResponse.newBuilder();
    if (descriptors != null && descriptors.size() > 0) {
      // Add the table descriptors to the response
      for (HTableDescriptor htd: descriptors) {
        builder.addTableSchema(htd.convert());
      }
    }
    return builder.build();
  } catch (IOException ioe) {
    throw new ServiceException(ioe);
  }
}
```






<br/>

## RpcClient

RpcClient的实现是RpcClientImpl, HBase有两个地方会调用RpcClientFactory.createClient方法来构建这个实例
1. ClusterConnection的实现HConnectionImplementation的构造函数里面
2. HRegionServer的线程初始化方法initializeThreads里面



<br/>
### RpcClientImpl#call

作为客户端调用一个Rpc的时候，最后都会调用RpcClientImpl#call方法, 这个方法大致流程如下
1. 用传入的参数构建Call实例
2. 调用getConnection获取到Rpc服务器的连接
3. 调用Connection#tracedWriteRequest把Call发送到远程服务器，这个方法在Connection#writeRequest外面裹了一层trace。
4. 设定timeout并调用call的wait方法等待结果的返回，正常情况下Connection线程里面调用readResponse读取RpcReponse的时候会调用Call#setResponse来唤醒call#wait, 因为setResponse里面调用了Call#callComplete, 而Call#callComplete里面有个notify方法

这些步骤里面比较复杂的是获取Connection并调用Connection的writeRequest, 下面看看Connection


<br/>
### RpcClientImp#Connection


Connection保持了到一个特定目标地址的socket连接，RpcClientImpl用connections（类型是PoolMap<ConnectionId, Connection>)来存储到各个不同节点的Connection，ConnectionId由用户、服务名称和地址三要素构成，也就是说RpcClientImpl对同一个用户、服务和地址只创建一个Connection。

用相同的用户同时发送到同一个地址上面同一个服务的多个Rpc调用会使用同一个Connection，这些call会被暂时存放在Connection.calls变量里面，它的类型是ConcurrentSkipListMap<Integer, Call>, 当从服务端获取到response之后会把call从这里面移除。


<br/>
**Connection#writeRequest**

在RpcClientImpl#call方法里面会调用tracedWriteRequest方法来发送Call到RPC服务器，实际是调用了writeRequest方法，这个方法的步骤如下:

1. 首先构建RequestHeader, 这个消息定义是在RPC.proto里面
2. 调用setupIOstreams来创建socket连接
3. 把call扔到calls里面
4. 调用IPCUtil.write把RequestHeader、调用参数和cellBlock写入到socket
5. 调用doNotifiy方法唤醒readResponde方法里面的等待。






<br/>
**创建socket并设置IO流**

在setupIOstreams里面，如发现socket是空，也就是连接还没创建，那么会做以下操作:

1. 调用setupConnection来创建socket并连接到远程服务器
2. 分别调用NetUtils.getInputStream和NetUtils.getOutputStream从socket获取输入和输出流
3. 调用Connection#writeConnectionHeaderPreamble往输出流里面写入Preamble，preamble总共6个字节，包括'HBas'（magic word), VERSION和AUTH_CODE。
4. 往输出流里面写入ConnectionHeader, 这个是在Connection的构建函数里面创建的，里面包含用户、目标服务、cell block编码和压缩方法和版本信息，这个消息同样也是定义在RPC.proto里面。从这里也可以看出ConnectionHeader只有在首次连接的时候发送一次，后续从已经创建socket的Connection发送RPC请求就不会发送ConnectionHeader。
5. 启动Connection的线程，并在里面运行Connection#run方法，这里面会等待直到被通知然后调用readReponse读取socket里面的内容。

<br/>

**读取答复**

在setupIOstreams的最后会启动Connection线程，即Connection#run方法，在这里面只做两件事情，一个是等待，另外一个是调用readResponse从socket输入流读取RPC的response。正常情况下writeRequest方法在把RpcCall写入socket之后会调用doNotify来唤醒Connection的主线程，因为这个时候calls已经不为空了，所以会调用readResponse方法，readResponse首先会调用一个readInt方法来读取流入的数据，如果服务端数据还没返回，这里就block住，直到服务器返回答复。






<br/>
## RpcServer详细架构

HBase的RpcServer架构跟Hadoop的RPC Server非常类似，Listener线程负责接收请求并分发给Reader，Reader负责读取并解析请求，Reader解析完成之后把Rpc请求放到Scheduler的队列里面，Scheduler里面有一堆Handler来处理请求，处理完成之后让Responder发送结果给Client。


![]({{ "/images/hbase-rpc-server.png" | absolute_url }})

<br/>

**创建RpcServer**

RpcServer的创建需要几个参数
1. HRegionServer
2. 名称
3. 服务列表，调用getServices方法获取服务列表
4. 绑定地址
5. 配置Configuration
6. SimpleRpcScheduler的实例， 调用SimpleRpcSchedulerFactory的create方法生成


<br/>
**Listener**

Listener是一个线程，RpcServer启动的时候会启动这个线程，主线程，即Listener#run方法里面一直循环调用selector#select()方法，一旦有返回，就会调用Listener#doAccept方法来处理selector里面选择的key。

在doAccept方法里面首先接收连接到这个socket的connection，然后调用getReader以轮询的方式从事先建好的多个Reader的获取一个，然后把当前的channel注册到这个Reader的selector里面, 并且创建一个Connection然后attach到注册返回的Key里面，后面Reader会取到这个Connection然后作读取处理。同时也会把这个Connection加到connectionList(List<Connection>)，connectionList维护了所有来自客户端的请求，主要用作后期清理工作。


<br/>

**Reader**

Listner构建的时候会初始化一堆Reader，Reader的个数由hbase.ipc.server.read.threadpool.size这个参数来决定，默认是10个。初始化之后，便在线程池里面启动这些Reader。

Reader也是个线程，也包含了一个selector，主线程启动之后便会调用selector的select方法，一旦有Listener把socket连接注册到这里的时候，便会调用Listener#doRead方法来处理这个请选择的key。

doRead方法首先调用key#attachment方法获取到Listener attach上来的Connection，然后调用Connection#readAndProcess来读取Rpc调用

1. 调用readPreamble来读取Preamble, 这个跟Client发送Preamble是遥相呼应的。这里也会进行是否已经读取的判断，因为Preamble只有连接之初才会发送的信息。
2. 接着会调用channelRead方法把接下来的数据全部读取到data这个ByteBuffer里面，读取完成之后调用Connection#process方法处理数据
3. 在Connection#process里面会根据是否开启Kerberos认真而选择saslReadAndProcess方法还是processOneRpc，假设我们没有开启Kerberos那么就会调用processOneRpc
4. 在processOneRpc里面会首先判断是否已经读取ConnectionHeader，如果没读取则会调用processConnectionHeader读取，不然则调用processRequest来处理RPC请求。
5. 在processConnectionHeader里面会读取并解析ConnectionHeader，接着提取出ServiceName，然后用用这个ServiceName从RPCServer初始化的时候注册的所有Service列表里面找到对应的service，这里为止就确认了这个Connection对应的是哪个service。后面还设置了cellblock的编码方式以及用户信息。
6. 在processRequest里面首先从buffer里面读取RequestHeader, 接着用RequestHeader里面的methodName去service里面获取methodDescriptor，然后接着从buffer里面读取参数，最后用这些读取到的信息构建RpcServer里面的Call,然后再用Call去构建CallRunner, 并调用Scheduler dispath方法把这个CallRunner分发到RpcExecutor上面去处理。


<br/>

**SimpleRpcScheduler**

当前默认使用的RpcSchdueler实现是SimpleRpcScheduler, 里面维护了3个handler的资源池子来分别处理一般的RPC调用、高优先级RPC调用和Replication调用。

SimpleRpcScheduler在RpcServer构建过程中就已经通过调用SimpleRpcSchedulerFactory#create方法被构建, 默认情况下在构建的过程中会生成三个FifoWithFastPathBalancedQueueRpcExecutor，分别来处理一般RPC、高优先级RPC和复制RPC调用。

跟Listener一样，Scheduler也是随着RpcServer启动而启动，即调用start()方法，启动过程中会调用下面的3个RpcExecutor的start方法启动这些Executor。
当Reader调用RpcScheduler#dispath方法的时候，会根据Rpc Call所带入的信息把RpcCall分发到对应的Executor上面。


<br/>

**RpcExecutor**

RpcExecutor的默认实现是FifoWithFastPathBalancedQueueRpcExecutor，里面包含了一堆handler，还有一些队列，有多个队列的时候会有一个队列Balancer来协调队列处理节奏。

RpcExecutor的初始化

1. RpcExecutor里面初始化一个Handler列表(List<Handler>), 注意这里并没有初始化Handler，只是创建一个列表。handler的具体数量由参数控制：
    - 高优先级Executor的handler由hbase.regionserver.metahandler.count参数控制，默认20
    - 复制Executor的handler数量由hbase.regionserver.replication.handler.count控制，默认3
    - 一般RpcExecutor的handler数量由hbase.regionserver.replication.handler.count来控制，默认30
2. BalancedQueueRpcExecutor初始化List<BlockingQueue<CallRunner>> queues
3. 根据numQueues获取QueueBalancer
4. 初始化队列放到queues里面。




<br/>
RpcExecutor的启动主要是调用了RpcExecutor#startHandlers来启动Handlers, 这里会调用getHandler来初始化hanlder，在FifoWithFastPathBalancedQueueRpcExecutor的实现里面，初始化的是FastPathHandler。另外这里可以看到每个Handler会获取到一个队列，而当由多个队列存在时，会以轮训的方式来分配。 最后调用Handler#start来启动Handler。

```java
protected void startHandlers(final String nameSuffix, final int numHandlers,
    final List<BlockingQueue<CallRunner>> callQueues,
    final int qindex, final int qsize, final int port,
    final AtomicInteger activeHandlerCount) {
  final String threadPrefix = name + Strings.nullToEmpty(nameSuffix);
  double handlerFailureThreshhold =
      conf == null ? 1.0 : conf.getDouble(HConstants.REGION_SERVER_HANDLER_ABORT_ON_ERROR_PERCENT,
        HConstants.DEFAULT_REGION_SERVER_HANDLER_ABORT_ON_ERROR_PERCENT);
  for (int i = 0; i < numHandlers; i++) {
    final int index = qindex + (i % qsize);
    String name = "RpcServer." + threadPrefix + ".handler=" + handlers.size() + ",queue=" +
        index + ",port=" + port;
    Handler handler = getHandler(name, handlerFailureThreshhold, callQueues.get(index),
      activeHandlerCount);
    handler.start();
    LOG.debug("Started " + name);
    handlers.add(handler);
  }
}
```


<br/>

当Handler启动之后马上就会调用getCallRunner来获取callRunner, getCallRunner的模式实现是从队列里面获取一个CallRunner， 但是FastPathHandler覆盖了该方法。可以看到里面首先会从队列里面去poll一下，因为这个时候刚启动队列肯定是空的，所以直接返回null。接着会把当前handler对象塞入fastPathHandlerStach, 并且调用seamphore#acquire方法，因为semaphore在FastPathHandler构建的时候已经调用drainPermits()把原来为1的permits用过了，所以这时候线程就在这里block住了。
```java
protected CallRunner getCallRunner() throws InterruptedException {
  // Get a callrunner if one in the Q.
  CallRunner cr = this.q.poll();
  if (cr == null) {
    // Else, if a fastPathHandlerStack present and no callrunner in Q, register ourselves for
    // the fastpath handoff done via fastPathHandlerStack.
    if (this.fastPathHandlerStack != null) {
      this.fastPathHandlerStack.push(this);
      this.semaphore.acquire();
      cr = this.loadedCallRunner;
    } else {
      // No fastpath available. Block until a task comes available.
      cr = super.getCallRunner();
    }
  }
  return cr;
}
```

<br/>

当Scheduler调用FifoWithFastPathBalancedQueueRpcExecutor的dispatch方法时，会调用popReadyHandler方法从fastPathHandlerStack获取一个handler，假设这个RPC请求是启动之后第一个过来的请求，那么stack里面默认是由30个handler（假设这是一个一般的Rpc处理Executor), 所以这时候就获取到了一个fastPathHandler, 并且马上会调用fastPathHandler#loadCallRunner方法。在loadCallRunner方法里面会传入的callRunner付给loadedCallRunner然后调用semaphore#release，这个时候Handler主线程里面之前被block住的地方就释放了，那么主线程获取到callRunner之后就调用CallRunner#call方法执行这个Request。

```java
boolean loadCallRunner(final CallRunner cr) {
  this.loadedCallRunner = cr;
  this.semaphore.release();
  return true;
}
```

<br/>
上面描述的这个过程就是所谓的FastPath，就是Reader所在的线程直接触发被block住的Handler线程。存放FastPath的Stack最多的时候就是全部Handler，当需要处理的请求多于全部handler的时候，请求就会进入队列，那么当handler发现队列不为空的时候就会先处理队列里面的请求。这个特性主要是为了保持handler线程处于活动状态以避免context的转换，从文档里面的描述可以看出这个特性是借鉴于kudu。



<br/>
最后RPC调用的实际执行是在CallRunner#run方法里面，run方法会进一步调用RpcServer#call方法来运行，因为这个时候server、method、param等都已经齐全了，所以直接调用service.callBlockingMethod方法来执行这个方法。执行完成之后会在CallRunner#run里面调用Call#setRespnose来构建response消息，response里面包含了resonseHeader、方法运行结果和cellblock(如果由的话)。最后会调用responser#doReponse方法把response发送给客户端。


<br/>
**Responder**

Responder线程是在RpcServer构建的时候被构建的并随着RpcServer的启动而启动. Responder主要包含了一个writeSelecotr，启动之后会调用它的select方法block住。

当Handler调用Responder#doResponse方法的时候，首先会尝试直接调用processResponse来发送response给Client，这里的条件是对应的Conneciton里面的respone队列为空。如果Connectind的responde队列里面已经由进程在写了，那么hanlder就不用等了，把处理放到队列里面，并且调用responder#registerForWrite方法来唤醒Reponder主线程进行异步写入。



<br/>
## 小结

总的来说Protocal Buffer为HBase RPC框架提供了高性能的RPC消息序列与反序列，另外也提供了RPC服务与方法的定义，使得RPC扩展变得非常清晰简单。



-------
注：hbase代码基于cloudera hbase, 版本cdh5-1.2.0_5.13.0
